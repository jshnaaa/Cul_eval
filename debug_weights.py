#!/usr/bin/env python3
"""
è°ƒè¯•æƒé‡åç§°æ˜ å°„
"""

from eval_culturebank import CultureBankModel

def debug_weight_mapping():
    print("ğŸ” è°ƒè¯•æƒé‡åç§°æ˜ å°„")
    print("=" * 50)

    # åˆå§‹åŒ–æ¨¡å‹
    culture_model = CultureBankModel()

    # åªåŠ è½½åŸºåº§æ¨¡å‹
    if not culture_model.load_base_model():
        print("âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # åªåŠ è½½adapteræƒé‡ï¼ˆä¸åº”ç”¨ï¼‰
    if not culture_model.load_adapter_weights():
        print("âŒ adapteræƒé‡åŠ è½½å¤±è´¥")
        return

    # åˆ†ææƒé‡åç§°
    print("\nğŸ“‹ LoRAæƒé‡åç§°æ ·ä¾‹:")
    lora_keys = list(culture_model.adapter_weights.keys())
    for i, key in enumerate(lora_keys[:10]):
        print(f"  {i+1}. {key}")

    print(f"\nğŸ“‹ åŸºåº§æ¨¡å‹æƒé‡åç§°æ ·ä¾‹:")
    base_keys = list(culture_model.base_model.state_dict().keys())
    for i, key in enumerate(base_keys[:10]):
        print(f"  {i+1}. {key}")

    # æµ‹è¯•åç§°è½¬æ¢
    print(f"\nğŸ”„ æµ‹è¯•åç§°è½¬æ¢:")
    test_lora_key = "base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight"
    expected_base_key = test_lora_key.replace('base_model.model.', '').replace('.lora_A.weight', '.weight')
    print(f"LoRAæƒé‡: {test_lora_key}")
    print(f"è½¬æ¢å: {expected_base_key}")
    print(f"æ˜¯å¦å­˜åœ¨: {expected_base_key in culture_model.base_model.state_dict()}")

    # æŸ¥æ‰¾åŒ¹é…çš„æƒé‡
    print(f"\nğŸ¯ æŸ¥æ‰¾åŒ¹é…çš„æƒé‡:")
    base_state_dict = culture_model.base_model.state_dict()
    matched_count = 0
    for key in lora_keys[:20]:  # åªæ£€æŸ¥å‰20ä¸ª
        if 'lora_A' in key:
            clean_name = key.replace('base_model.model.', '').replace('.lora_A.weight', '.weight')
            if clean_name in base_state_dict:
                print(f"  âœ… åŒ¹é…: {key} -> {clean_name}")
                matched_count += 1
            else:
                print(f"  âŒ æœªåŒ¹é…: {key} -> {clean_name}")

    print(f"\nğŸ“Š åŒ¹é…ç‡: {matched_count}/{min(20, len([k for k in lora_keys if 'lora_A' in k]))}")

if __name__ == "__main__":
    debug_weight_mapping()