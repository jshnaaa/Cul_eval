#!/usr/bin/env python3
"""
Llama Metaæ ¼å¼è½¬HFæ ¼å¼è½¬æ¢è„šæœ¬
åŸºäºtransformersåº“çš„å®˜æ–¹è½¬æ¢é€»è¾‘
"""

import argparse
import json
import os
import shutil
import torch
from pathlib import Path
from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer


def read_json(path):
    with open(path, "r") as f:
        return json.load(f)


def write_json(text, path):
    with open(path, "w") as f:
        json.dump(text, f, indent=2)


def write_model(model_path, input_base_path, model_size):
    """
    è½¬æ¢Metaæ ¼å¼çš„Llamaæ¨¡å‹åˆ°HFæ ¼å¼
    """
    os.makedirs(model_path, exist_ok=True)
    print(f"æ­£åœ¨è½¬æ¢æ¨¡å‹åˆ°: {model_path}")

    # è¯»å–åŸå§‹å‚æ•°
    params_path = os.path.join(input_base_path, "params.json")
    if not os.path.exists(params_path):
        print(f"âŒ æ‰¾ä¸åˆ° params.json æ–‡ä»¶: {params_path}")
        return False

    params = read_json(params_path)
    print(f"ğŸ“‹ è¯»å–æ¨¡å‹å‚æ•°: {params}")

    # åˆ›å»ºHFé…ç½®
    config = LlamaConfig(
        vocab_size=params.get("vocab_size", 32000),
        hidden_size=params["dim"],
        intermediate_size=params.get("ffn_dim_multiplier", 1) * params["dim"] * 8 // 3,
        num_hidden_layers=params["n_layers"],
        num_attention_heads=params["n_heads"],
        num_key_value_heads=params.get("n_kv_heads", params["n_heads"]),
        max_position_embeddings=params.get("max_seq_len", 4096),
        rms_norm_eps=params.get("norm_eps", 1e-5),
        rope_theta=params.get("rope_theta", 10000.0),
        bos_token_id=1,
        eos_token_id=2,
        pad_token_id=0,
        tie_word_embeddings=False,
        torch_dtype="float16",
    )

    # ä¿å­˜é…ç½®
    config.save_pretrained(model_path)
    print("âœ… ä¿å­˜config.json")

    # åŠ è½½æƒé‡æ–‡ä»¶
    weight_file = os.path.join(input_base_path, "consolidated.00.pth")
    if not os.path.exists(weight_file):
        print(f"âŒ æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶: {weight_file}")
        return False

    print("ğŸ“¦ åŠ è½½åŸå§‹æƒé‡...")
    state_dict = torch.load(weight_file, map_location="cpu")

    # è½¬æ¢æƒé‡å‘½å
    print("ğŸ”„ è½¬æ¢æƒé‡å‘½å...")
    new_state_dict = {}

    # æƒé‡æ˜ å°„è§„åˆ™
    for key, value in state_dict.items():
        if key == "tok_embeddings.weight":
            new_state_dict["model.embed_tokens.weight"] = value
        elif key == "norm.weight":
            new_state_dict["model.norm.weight"] = value
        elif key == "output.weight":
            new_state_dict["lm_head.weight"] = value
        elif key.startswith("layers."):
            # å¤„ç†transformerå±‚
            layer_num = key.split(".")[1]
            if "attention.wq.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.self_attn.q_proj.weight"] = value
            elif "attention.wk.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.self_attn.k_proj.weight"] = value
            elif "attention.wv.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.self_attn.v_proj.weight"] = value
            elif "attention.wo.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.self_attn.o_proj.weight"] = value
            elif "attention_norm.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.input_layernorm.weight"] = value
            elif "feed_forward.w1.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.mlp.gate_proj.weight"] = value
            elif "feed_forward.w2.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.mlp.down_proj.weight"] = value
            elif "feed_forward.w3.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.mlp.up_proj.weight"] = value
            elif "ffn_norm.weight" in key:
                new_state_dict[f"model.layers.{layer_num}.post_attention_layernorm.weight"] = value

    # ä¿å­˜è½¬æ¢åçš„æƒé‡
    print("ğŸ’¾ ä¿å­˜è½¬æ¢åçš„æƒé‡...")
    torch.save(new_state_dict, os.path.join(model_path, "pytorch_model.bin"))
    print("âœ… ä¿å­˜pytorch_model.bin")

    # å¤„ç†tokenizer
    print("ğŸ”¤ å¤„ç†tokenizer...")
    tokenizer_model_path = os.path.join(input_base_path, "tokenizer.model")
    if os.path.exists(tokenizer_model_path):
        # å¤åˆ¶tokenizer.modelæ–‡ä»¶
        shutil.copy(tokenizer_model_path, os.path.join(model_path, "tokenizer.model"))

        # åˆ›å»ºtokenizeré…ç½®
        tokenizer_config = {
            "tokenizer_class": "LlamaTokenizer",
            "bos_token": "<s>",
            "eos_token": "</s>",
            "unk_token": "<unk>",
            "pad_token": None,
            "sp_model_kwargs": {},
            "spaces_between_special_tokens": False,
            "legacy": True
        }

        write_json(tokenizer_config, os.path.join(model_path, "tokenizer_config.json"))

        # åˆ›å»ºspecial_tokens_map
        special_tokens_map = {
            "bos_token": "<s>",
            "eos_token": "</s>",
            "unk_token": "<unk>"
        }
        write_json(special_tokens_map, os.path.join(model_path, "special_tokens_map.json"))

        print("âœ… ä¿å­˜tokenizeræ–‡ä»¶")

    print("ğŸ‰ è½¬æ¢å®Œæˆ!")
    return True


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_dir",
        help="Metaæ ¼å¼æ¨¡å‹ç›®å½•è·¯å¾„",
        default="./Llama-2-7b-chat"
    )
    parser.add_argument(
        "--output_dir",
        help="HFæ ¼å¼è¾“å‡ºç›®å½•è·¯å¾„",
        default="./Llama-2-7b-chat-hf"
    )
    parser.add_argument(
        "--model_size",
        help="æ¨¡å‹å¤§å°",
        default="7B"
    )
    args = parser.parse_args()

    print("ğŸ¦™ Llama Metaæ ¼å¼ -> HFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 50)
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“ æ¨¡å‹å¤§å°: {args.model_size}")
    print("=" * 50)

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return

    required_files = ["params.json", "consolidated.00.pth", "tokenizer.model"]
    for file in required_files:
        file_path = os.path.join(args.input_dir, file)
        if not os.path.exists(file_path):
            print(f"âŒ å¿…éœ€æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

    # æ‰§è¡Œè½¬æ¢
    success = write_model(args.output_dir, args.input_dir, args.model_size)

    if success:
        print(f"\nğŸ‰ è½¬æ¢æˆåŠŸ! HFæ ¼å¼æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
        print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
        for file in os.listdir(args.output_dir):
            print(f"  - {file}")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥!")


if __name__ == "__main__":
    main()