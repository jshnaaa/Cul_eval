#!/usr/bin/env python3
"""
ç®€åŒ–æµ‹è¯•ç‰ˆæœ¬ - åªæµ‹è¯•æ¨¡å‹åŠ è½½ï¼Œä¸è¿›è¡Œç”Ÿæˆ
"""

import json
import os
import torch
import sentencepiece as spm
from eval_llama2 import Llama2Model

def main():
    print("ğŸ§ª ç®€åŒ–æµ‹è¯•ï¼šåªåŠ è½½æ¨¡å‹ï¼Œä¸ç”Ÿæˆæ–‡æœ¬")
    print("=" * 50)

    # åˆå§‹åŒ–æ¨¡å‹
    model_path = "./Llama-2-7b-chat"
    llama_model = Llama2Model(model_path)

    # åŠ è½½æ¨¡å‹
    if not llama_model.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")

    # æµ‹è¯•tokenizer
    test_text = "hello!"
    tokens = llama_model.encode(test_text)
    decoded = llama_model.decode(tokens)

    print(f"ğŸ“ åŸæ–‡: {test_text}")
    print(f"ğŸ”¢ ç¼–ç : {tokens}")
    print(f"ğŸ“ è§£ç : {decoded}")

    # æµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆä¸ç”Ÿæˆï¼‰
    print("ğŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­...")
    chat_prompt = llama_model.format_chat_prompt("hello")
    tokens = llama_model.encode(chat_prompt)
    tokens_tensor = torch.tensor([tokens], dtype=torch.long).to(llama_model.device)

    print(f"ğŸ“ è¾“å…¥tokensé•¿åº¦: {tokens_tensor.shape[1]}")

    with torch.no_grad():
        logits = llama_model.model.forward(tokens_tensor, 0)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {logits.shape}")

    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")

if __name__ == "__main__":
    main()