#!/usr/bin/env python3
"""
CultureBankæ¨¡å‹è¯„ä¼°è„šæœ¬ - ç®€åŒ–ç‰ˆ
é€‚ç”¨äºå†…å­˜æœ‰é™æˆ–ä¾èµ–é—®é¢˜çš„æƒ…å†µ

æœ€å°ä¾èµ–:
pip install transformers peft torch

ä½¿ç”¨æ–¹æ³•:
python eval_culturebank_simple.py
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")


def load_culturebank_model(model_path="./sft_preference_v0.3"):
    """
    åŠ è½½CultureBankæ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬
    """
    print("ğŸš€ åŠ è½½CultureBankæ¨¡å‹...")

    base_model_name = "meta-llama/Llama-2-7b-chat-hf"

    try:
        # åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆä½¿ç”¨CPUæˆ–è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼‰
        print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        # åŠ è½½LoRA adapter
        print(f"ğŸ“¥ åŠ è½½LoRA adapter: {model_path}")
        model = PeftModel.from_pretrained(base_model, model_path)

        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, tokenizer

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None


def generate_response(model, tokenizer, message):
    """
    ç”Ÿæˆæ¨¡å‹å›åº”
    """
    # Llama-2 chatæ ¼å¼
    system_msg = "You are a helpful assistant with cultural knowledge."
    prompt = f"<s>[INST] <<SYS>>\n{system_msg}\n<</SYS>>\n\n{message} [/INST]"

    # ç¼–ç è¾“å…¥
    inputs = tokenizer.encode(prompt, return_tensors="pt")

    # ç”Ÿæˆå›åº”
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # è§£ç å¹¶æå–å›åº”
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_response[len(prompt):].strip()

    return response


def main():
    """
    ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆè¯„ä¼°
    """
    print("ğŸ›ï¸  CultureBankæ¨¡å‹è¯„ä¼°å™¨ (ç®€åŒ–ç‰ˆ)")
    print("=" * 50)

    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_culturebank_model()

    if model is None or tokenizer is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œé€€å‡º")
        return

    # æµ‹è¯•ç¤ºä¾‹
    test_message = "ä½ å¥½"

    print(f"\nğŸ‘¤ æµ‹è¯•è¾“å…¥: {test_message}")
    print("ğŸ¤– CultureBankå›åº”:")
    print("-" * 30)

    response = generate_response(model, tokenizer, test_message)
    print(response)

    print("-" * 30)
    print("âœ… æµ‹è¯•å®Œæˆ!")

    # ç®€å•äº¤äº’
    print("\nğŸ’¬ ç®€å•äº¤äº’æµ‹è¯• (è¾“å…¥ 'quit' é€€å‡º)")
    while True:
        user_input = input("\nğŸ‘¤ æ‚¨: ").strip()

        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§!")
            break

        if user_input:
            print("ğŸ¤– CultureBank:")
            response = generate_response(model, tokenizer, user_input)
            print(response)


if __name__ == "__main__":
    main()