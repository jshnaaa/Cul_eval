#!/usr/bin/env python3
"""
CultureBankæ¨¡å‹åŠ è½½è„šæœ¬ - ç›´æ¥åŠ è½½Metaæ ¼å¼åŸºåº§æ¨¡å‹ + LoRA adapter
åŸºäºeval_llama2.pyçš„æˆåŠŸåŠ è½½æ–¹å¼
"""

import json
import os
import torch
import torch.nn.functional as F
import sentencepiece as spm
from eval_llama2 import Llama2Model, Transformer, ModelArgs
from typing import Dict, Any

# å°è¯•å¯¼å…¥safetensorsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ
try:
    from safetensors import safe_open
    HAS_SAFETENSORS = True
except ImportError:
    print("âš ï¸  safetensorsæœªå®‰è£…ï¼Œå°†åªæ”¯æŒ.binæ ¼å¼çš„adapter")
    HAS_SAFETENSORS = False


class CultureBankModel:
    def __init__(self, base_model_path="./Llama-2-7b-chat", adapter_path="./CultureBank-Llama2-SFT/sft_preference_v0.3"):
        """
        åˆå§‹åŒ–CultureBankæ¨¡å‹

        Args:
            base_model_path: Metaæ ¼å¼åŸºåº§æ¨¡å‹è·¯å¾„
            adapter_path: LoRA adapterè·¯å¾„
        """
        self.base_model_path = base_model_path
        self.adapter_path = adapter_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åŸºåº§æ¨¡å‹ç»„ä»¶
        self.base_model = None
        self.tokenizer = None

        # adapteræƒé‡
        self.adapter_weights = {}

    def load_base_model(self):
        """åŠ è½½Metaæ ¼å¼çš„åŸºåº§æ¨¡å‹"""
        print("ğŸ¦™ åŠ è½½Metaæ ¼å¼åŸºåº§æ¨¡å‹...")

        # å¤ç”¨eval_llama2.pyçš„åŠ è½½é€»è¾‘
        llama_loader = Llama2Model(self.base_model_path)

        if not llama_loader.load_model():
            print("âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥")
            return False

        # è·å–åŠ è½½å¥½çš„æ¨¡å‹å’Œtokenizer
        self.base_model = llama_loader.model
        self.tokenizer = llama_loader.tokenizer

        print("âœ… åŸºåº§æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return True

    def load_adapter_weights(self):
        """åŠ è½½LoRA adapteræƒé‡"""
        print("ğŸ“¦ åŠ è½½LoRA adapteræƒé‡...")

        if not os.path.exists(self.adapter_path):
            print(f"âŒ Adapterè·¯å¾„ä¸å­˜åœ¨: {self.adapter_path}")
            return False

        try:
            # æŸ¥æ‰¾adapteræƒé‡æ–‡ä»¶
            adapter_files = []
            for file in os.listdir(self.adapter_path):
                if file.endswith('.safetensors') or file.endswith('.bin'):
                    adapter_files.append(file)

            if not adapter_files:
                print(f"âŒ åœ¨{self.adapter_path}ä¸­æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶(.safetensorsæˆ–.bin)")
                return False

            print(f"ğŸ“ æ‰¾åˆ°adapteræ–‡ä»¶: {adapter_files}")

            # åŠ è½½adapteræƒé‡
            for file in adapter_files:
                file_path = os.path.join(self.adapter_path, file)

                if file.endswith('.safetensors') and HAS_SAFETENSORS:
                    # åŠ è½½safetensorsæ ¼å¼
                    with safe_open(file_path, framework="pt", device="cpu") as f:
                        for key in f.keys():
                            self.adapter_weights[key] = f.get_tensor(key)
                            print(f"  ğŸ“‹ åŠ è½½æƒé‡: {key}, å½¢çŠ¶: {self.adapter_weights[key].shape}")
                elif file.endswith('.safetensors') and not HAS_SAFETENSORS:
                    print(f"  âš ï¸  è·³è¿‡safetensorsæ–‡ä»¶ï¼ˆéœ€è¦å®‰è£…safetensorsåŒ…ï¼‰: {file}")
                elif file.endswith('.bin'):
                    # åŠ è½½pytorchæ ¼å¼
                    weights = torch.load(file_path, map_location="cpu")
                    for key, value in weights.items():
                        self.adapter_weights[key] = value
                        print(f"  ğŸ“‹ åŠ è½½æƒé‡: {key}, å½¢çŠ¶: {value.shape}")

            print(f"âœ… æˆåŠŸåŠ è½½{len(self.adapter_weights)}ä¸ªadapteræƒé‡")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½adapteræƒé‡å¤±è´¥: {str(e)}")
            return False

    def apply_lora_weights(self):
        """å°†LoRAæƒé‡åº”ç”¨åˆ°åŸºåº§æ¨¡å‹"""
        print("ğŸ”§ åº”ç”¨LoRAæƒé‡åˆ°åŸºåº§æ¨¡å‹...")

        try:
            # è·å–åŸºåº§æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            base_state_dict = self.base_model.state_dict()

            # åˆ†ç»„LoRAæƒé‡ï¼šå°†lora_Aå’Œlora_Bé…å¯¹
            lora_pairs = {}
            for key in self.adapter_weights.keys():
                if 'lora_A' in key:
                    base_name = key.replace('.lora_A.weight', '')
                    lora_pairs[base_name] = lora_pairs.get(base_name, {})
                    lora_pairs[base_name]['A'] = key
                elif 'lora_B' in key:
                    base_name = key.replace('.lora_B.weight', '')
                    lora_pairs[base_name] = lora_pairs.get(base_name, {})
                    lora_pairs[base_name]['B'] = key

            print(f"ğŸ“Š æ‰¾åˆ°{len(lora_pairs)}ä¸ªLoRAæƒé‡å¯¹")

            # åº”ç”¨LoRAæƒé‡ï¼šW_new = W_base + lora_B @ lora_A
            applied_count = 0
            for base_name, pair in lora_pairs.items():
                if 'A' in pair and 'B' in pair:
                    # æ„å»ºåŸºåº§æ¨¡å‹æƒé‡åç§°
                    base_key = base_name + '.weight'

                    if base_key in base_state_dict:
                        try:
                            # è·å–LoRAæƒé‡
                            lora_A = self.adapter_weights[pair['A']].to(self.device)
                            lora_B = self.adapter_weights[pair['B']].to(self.device)

                            # è®¡ç®—LoRAå¢é‡ï¼šdelta_W = lora_B @ lora_A
                            delta_W = torch.matmul(lora_B, lora_A)

                            # è·å–åŸºåº§æƒé‡
                            base_weight = base_state_dict[base_key].to(self.device)

                            # åˆå¹¶æƒé‡ï¼šW_new = W_base + delta_W
                            new_weight = base_weight + delta_W

                            # æ›´æ–°æ¨¡å‹æƒé‡
                            base_state_dict[base_key].copy_(new_weight)

                            print(f"  âœ… åº”ç”¨LoRA: {base_name}")
                            applied_count += 1

                        except Exception as e:
                            print(f"  âŒ åº”ç”¨LoRAå¤±è´¥ {base_name}: {str(e)}")
                    else:
                        print(f"  âš ï¸  æœªæ‰¾åˆ°å¯¹åº”åŸºåº§æƒé‡: {base_key}")

            print(f"âœ… æˆåŠŸåº”ç”¨{applied_count}ä¸ªLoRAæƒé‡å¯¹")
            return True

        except Exception as e:
            print(f"âŒ åº”ç”¨LoRAæƒé‡å¤±è´¥: {str(e)}")
            return False

    def load_model(self):
        """åŠ è½½å®Œæ•´çš„CultureBankæ¨¡å‹"""
        print("ğŸ¯ å¼€å§‹åŠ è½½CultureBankæ¨¡å‹...")

        # 1. åŠ è½½åŸºåº§æ¨¡å‹
        if not self.load_base_model():
            return False

        # 2. åŠ è½½adapteræƒé‡
        if not self.load_adapter_weights():
            return False

        # 3. åº”ç”¨LoRAæƒé‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if not self.apply_lora_weights():
            return False

        print("ğŸ‰ CultureBankæ¨¡å‹åŠ è½½å®Œæˆ!")
        return True

    def format_chat_prompt(self, message: str, system_message: str = None):
        """æ ¼å¼åŒ–Llama-2 chatæ ¼å¼"""
        if system_message is None:
            system_message = "You are CultureBank, a helpful assistant with deep cultural knowledge from around the world."

        prompt = f"<s>[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{message} [/INST]"
        return prompt

    def encode(self, text: str):
        """ç¼–ç æ–‡æœ¬ä¸ºtoken"""
        return self.tokenizer.encode(text, out_type=int)

    def decode(self, tokens):
        """è§£ç tokenä¸ºæ–‡æœ¬"""
        return self.tokenizer.decode(tokens)

    def generate(self, prompt: str, max_tokens: int = 100, temperature: float = 0.7):
        """ç”Ÿæˆæ–‡æœ¬å“åº”"""
        if self.base_model is None or self.tokenizer is None:
            return "âŒ æ¨¡å‹æœªåŠ è½½"

        # ç¼–ç è¾“å…¥
        tokens = self.encode(prompt)
        tokens = torch.tensor([tokens], dtype=torch.long).to(self.device)

        print(f"ğŸ“ è¾“å…¥tokensé•¿åº¦: {tokens.shape[1]}")

        generated_tokens = []

        # ä½¿ç”¨ä¸eval_llama2.pyç›¸åŒçš„ç”Ÿæˆé€»è¾‘
        with torch.no_grad():
            current_tokens = tokens.clone()

            for i in range(max_tokens):
                try:
                    # å‰å‘ä¼ æ’­
                    logits = self.base_model.forward(current_tokens, 0)

                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    last_logits = logits[0, -1, :]

                    # åº”ç”¨temperature
                    if temperature > 0:
                        last_logits = last_logits / temperature
                        probs = F.softmax(last_logits, dim=-1)
                        next_token_id = torch.multinomial(probs, num_samples=1).item()
                    else:
                        next_token_id = torch.argmax(last_logits, dim=-1).item()

                    # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸtoken
                    if next_token_id == 2:  # </s> token
                        break

                    generated_tokens.append(next_token_id)

                    # åˆ›å»ºæ–°çš„tokenå¹¶æ‹¼æ¥åˆ°åºåˆ—
                    next_token_tensor = torch.tensor([[next_token_id]], dtype=torch.long, device=self.device)
                    current_tokens = torch.cat([current_tokens, next_token_tensor], dim=1)

                except Exception as e:
                    print(f"âŒ ç”Ÿæˆæ­¥éª¤ {i+1} å¤±è´¥: {str(e)}")
                    break

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        if generated_tokens:
            generated_text = self.decode(generated_tokens)
            return generated_text.strip()
        else:
            return ""


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›ï¸  CultureBankæ¨¡å‹åŠ è½½æµ‹è¯•")
    print("=" * 60)

    # åˆå§‹åŒ–CultureBankæ¨¡å‹
    culture_model = CultureBankModel()

    # åŠ è½½æ¨¡å‹
    if not culture_model.load_model():
        print("âŒ CultureBankæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return

    # æµ‹è¯•ç”¨ä¾‹
    test_message = "Tell me about Chinese New Year traditions."
    print(f"\nğŸ‘¤ ç”¨æˆ·è¾“å…¥: {test_message}")

    # æ ¼å¼åŒ–ä¸ºchatæ ¼å¼
    chat_prompt = culture_model.format_chat_prompt(test_message)
    print(f"ğŸ”¤ æ ¼å¼åŒ–æç¤º: {chat_prompt[:100]}...")

    # ç”Ÿæˆå›å¤
    print("ğŸ¤– CultureBankå›åº”:")
    response = culture_model.generate(chat_prompt, max_tokens=100, temperature=0.7)
    print(response)

    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()