#!/usr/bin/env python3
"""
CultureBankæ¨¡å‹è¯„æµ‹è„šæœ¬ - æ”¯æŒå¤šæ•°æ®é›†è¯„æµ‹
åŸºäºeval_llama2.pyçš„æˆåŠŸåŠ è½½æ–¹å¼ + eval_spa.pyçš„è¯„æµ‹é€»è¾‘
"""

import json
import os
import re
import argparse
import torch
import torch.nn.functional as F
import sentencepiece as spm
from eval_llama2 import Llama2Model, Transformer, ModelArgs
from typing import Dict, Any, List
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from datetime import datetime

# å°è¯•å¯¼å…¥safetensorsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ
try:
    from safetensors import safe_open
    HAS_SAFETENSORS = True
except ImportError:
    print("âš ï¸  safetensorsæœªå®‰è£…ï¼Œå°†åªæ”¯æŒ.binæ ¼å¼çš„adapter")
    HAS_SAFETENSORS = False


class CultureBankEvaluator:
    def __init__(self, base_model_path="./Llama-2-7b-chat", adapter_path="./CultureBank-Llama2-SFT/sft_preference_v0.3"):
        """
        åˆå§‹åŒ–CultureBankè¯„æµ‹å™¨

        Args:
            base_model_path: Metaæ ¼å¼åŸºåº§æ¨¡å‹è·¯å¾„
            adapter_path: LoRA adapterè·¯å¾„
        """
        self.base_model_path = base_model_path
        self.adapter_path = adapter_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åŸºåº§æ¨¡å‹ç»„ä»¶
        self.base_model = None
        self.tokenizer = None

        # adapteræƒé‡
        self.adapter_weights = {}

    def convert_lora_to_base_key(self, lora_key: str) -> str:
        """
        å°†LoRAæƒé‡åç§°è½¬æ¢ä¸ºåŸºåº§æ¨¡å‹æƒé‡åç§°

        Args:
            lora_key: LoRAæƒé‡åç§°ï¼Œå¦‚ 'base_model.model.model.layers.0.self_attn.q_proj'

        Returns:
            åŸºåº§æ¨¡å‹æƒé‡åç§°ï¼Œå¦‚ 'layers.0.attention.wq.weight'
        """
        # ç§»é™¤å‰ç¼€
        if lora_key.startswith('base_model.model.'):
            clean_name = lora_key.replace('base_model.model.', '')
        else:
            clean_name = lora_key

        # HFæ ¼å¼åˆ°Meta Llamaæ ¼å¼çš„æ˜ å°„
        # model.layers.X.self_attn.Y_proj -> layers.X.attention.wY
        if 'model.layers.' in clean_name and 'self_attn.' in clean_name:
            import re
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å±‚å·å’ŒæŠ•å½±ç±»å‹
            pattern = r'model\.layers\.(\d+)\.self_attn\.([qkvo])_proj'
            match = re.search(pattern, clean_name)

            if match:
                layer_idx = match.group(1)
                proj_type = match.group(2)

                # æ˜ å°„æŠ•å½±ç±»å‹
                proj_mapping = {
                    'q': 'wq',
                    'k': 'wk',
                    'v': 'wv',
                    'o': 'wo'
                }

                if proj_type in proj_mapping:
                    return f"layers.{layer_idx}.attention.{proj_mapping[proj_type]}.weight"

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œè¿”å›åŸå§‹åç§°åŠ .weight
        return clean_name + '.weight'

    def load_base_model(self):
        """åŠ è½½Metaæ ¼å¼çš„åŸºåº§æ¨¡å‹"""
        print("ğŸ¦™ åŠ è½½Metaæ ¼å¼åŸºåº§æ¨¡å‹...")

        # å¤ç”¨eval_llama2.pyçš„åŠ è½½é€»è¾‘
        llama_loader = Llama2Model(self.base_model_path)

        if not llama_loader.load_model():
            print("âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥")
            return False

        # è·å–åŠ è½½å¥½çš„æ¨¡å‹å’Œtokenizer
        self.base_model = llama_loader.model
        self.tokenizer = llama_loader.tokenizer

        print("âœ… åŸºåº§æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return True

    def load_adapter_weights(self):
        """åŠ è½½LoRA adapteræƒé‡"""
        print("ğŸ“¦ åŠ è½½LoRA adapteræƒé‡...")

        if not os.path.exists(self.adapter_path):
            print(f"âŒ Adapterè·¯å¾„ä¸å­˜åœ¨: {self.adapter_path}")
            return False

        try:
            # æŸ¥æ‰¾adapteræƒé‡æ–‡ä»¶ï¼Œæ’é™¤éæƒé‡æ–‡ä»¶
            adapter_files = []
            excluded_files = ['training_args.bin', 'optimizer.bin', 'scheduler.bin', 'rng_state.pth']

            for file in os.listdir(self.adapter_path):
                if file.endswith('.safetensors'):
                    if 'adapter' in file.lower() or 'lora' in file.lower():
                        adapter_files.append(file)
                elif file.endswith('.bin') and file not in excluded_files:
                    # æ’é™¤å·²çŸ¥çš„éæƒé‡æ–‡ä»¶
                    if 'adapter' in file.lower() or 'lora' in file.lower():
                        adapter_files.append(file)

            if not adapter_files:
                print(f"âŒ åœ¨{self.adapter_path}ä¸­æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶(.safetensorsæˆ–.bin)")
                return False

            print(f"ğŸ“ æ‰¾åˆ°adapteræ–‡ä»¶: {adapter_files}")

            # åŠ è½½adapteræƒé‡
            for file in adapter_files:
                file_path = os.path.join(self.adapter_path, file)

                try:
                    if file.endswith('.safetensors') and HAS_SAFETENSORS:
                        # åŠ è½½safetensorsæ ¼å¼
                        with safe_open(file_path, framework="pt", device="cpu") as f:
                            for key in f.keys():
                                self.adapter_weights[key] = f.get_tensor(key)
                    elif file.endswith('.bin'):
                        # åŠ è½½pytorchæ ¼å¼
                        weights = torch.load(file_path, map_location="cpu")

                        # æ£€æŸ¥æ˜¯å¦æ˜¯å­—å…¸æ ¼å¼çš„æƒé‡æ–‡ä»¶
                        if isinstance(weights, dict) and hasattr(weights, 'items'):
                            for key, value in weights.items():
                                if isinstance(value, torch.Tensor):
                                    self.adapter_weights[key] = value

                except Exception as e:
                    print(f"  âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file}: {str(e)}")
                    continue

            print(f"âœ… æˆåŠŸåŠ è½½{len(self.adapter_weights)}ä¸ªadapteræƒé‡")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½adapteræƒé‡å¤±è´¥: {str(e)}")
            return False

    def apply_lora_weights(self):
        """å°†LoRAæƒé‡åº”ç”¨åˆ°åŸºåº§æ¨¡å‹"""
        print("ğŸ”§ åº”ç”¨LoRAæƒé‡åˆ°åŸºåº§æ¨¡å‹...")

        try:
            # è·å–åŸºåº§æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            base_state_dict = self.base_model.state_dict()

            # åˆ†ç»„LoRAæƒé‡ï¼šå°†lora_Aå’Œlora_Bé…å¯¹
            lora_pairs = {}
            for key in self.adapter_weights.keys():
                if 'lora_A' in key:
                    base_name = key.replace('.lora_A.weight', '')
                    lora_pairs[base_name] = lora_pairs.get(base_name, {})
                    lora_pairs[base_name]['A'] = key
                elif 'lora_B' in key:
                    base_name = key.replace('.lora_B.weight', '')
                    lora_pairs[base_name] = lora_pairs.get(base_name, {})
                    lora_pairs[base_name]['B'] = key

            print(f"ğŸ“Š æ‰¾åˆ°{len(lora_pairs)}ä¸ªLoRAæƒé‡å¯¹")

            # åº”ç”¨LoRAæƒé‡ï¼šW_new = W_base + lora_B @ lora_A
            applied_count = 0
            for base_name, pair in lora_pairs.items():
                if 'A' in pair and 'B' in pair:
                    # è½¬æ¢LoRAæƒé‡åç§°åˆ°åŸºåº§æ¨¡å‹æƒé‡åç§°
                    base_key = self.convert_lora_to_base_key(base_name)

                    if base_key in base_state_dict:
                        try:
                            # è·å–LoRAæƒé‡
                            lora_A = self.adapter_weights[pair['A']].to(self.device)
                            lora_B = self.adapter_weights[pair['B']].to(self.device)

                            # è®¡ç®—LoRAå¢é‡ï¼šdelta_W = lora_B @ lora_A
                            delta_W = torch.matmul(lora_B, lora_A)

                            # è·å–åŸºåº§æƒé‡
                            base_weight = base_state_dict[base_key].to(self.device)

                            # åˆå¹¶æƒé‡ï¼šW_new = W_base + delta_W
                            new_weight = base_weight + delta_W

                            # æ›´æ–°æ¨¡å‹æƒé‡
                            base_state_dict[base_key].copy_(new_weight)

                            applied_count += 1

                        except Exception as e:
                            print(f"  âŒ åº”ç”¨LoRAå¤±è´¥ {base_name}: {str(e)}")

            print(f"âœ… æˆåŠŸåº”ç”¨{applied_count}ä¸ªLoRAæƒé‡å¯¹")
            return True

        except Exception as e:
            print(f"âŒ åº”ç”¨LoRAæƒé‡å¤±è´¥: {str(e)}")
            return False

    def load_model(self):
        """åŠ è½½å®Œæ•´çš„CultureBankæ¨¡å‹"""
        print("ğŸ¯ å¼€å§‹åŠ è½½CultureBankæ¨¡å‹...")

        # 1. åŠ è½½åŸºåº§æ¨¡å‹
        if not self.load_base_model():
            return False

        # 2. åŠ è½½adapteræƒé‡
        if not self.load_adapter_weights():
            return False

        # 3. åº”ç”¨LoRAæƒé‡
        if not self.apply_lora_weights():
            return False

        print("ğŸ‰ CultureBankæ¨¡å‹åŠ è½½å®Œæˆ!")
        return True

    def format_chat_prompt(self, message: str, system_message: str = None):
        """æ ¼å¼åŒ–Llama-2 chatæ ¼å¼"""
        if system_message is None:
            system_message = "You are CultureBank, a helpful assistant with deep cultural knowledge from around the world."

        prompt = f"<s>[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{message} [/INST]"
        return prompt

    def encode(self, text: str):
        """ç¼–ç æ–‡æœ¬ä¸ºtoken"""
        return self.tokenizer.encode(text, out_type=int)

    def decode(self, tokens):
        """è§£ç tokenä¸ºæ–‡æœ¬"""
        return self.tokenizer.decode(tokens)

    def generate_response(self, instruction: str, max_new_tokens: int = 1024, temperature: float = 0.0):
        """
        ç”Ÿæˆæ¨¡å‹å“åº” - é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œä¼˜å…ˆæ•°å­—

        Args:
            instruction: è¾“å…¥æŒ‡ä»¤
            max_new_tokens: æœ€å¤§æ–°ç”Ÿæˆtokenæ•°é‡
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„å›å¤
        """
        if self.base_model is None or self.tokenizer is None:
            return "âŒ æ¨¡å‹æœªåŠ è½½"

        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_chat_prompt(instruction)

        # ç¼–ç è¾“å…¥
        tokens = self.encode(prompt)

        # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„context
        if len(tokens) > 2000:
            tokens = tokens[-2000:]  # åªä¿ç•™æœ€å2000ä¸ªtoken

        tokens = torch.tensor([tokens], dtype=torch.long).to(self.device)

        generated_tokens = []

        # æ¢å¤å·¥ä½œçš„ç”Ÿæˆé€»è¾‘ï¼Œä½†æ·»åŠ ä¸¥æ ¼æ§åˆ¶
        with torch.no_grad():
            current_tokens = tokens.clone()

            for i in range(max_new_tokens):
                try:
                    # å‰å‘ä¼ æ’­
                    logits = self.base_model.forward(current_tokens, 0)

                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    last_logits = logits[0, -1, :]

                    # è´ªå©ªè§£ç 
                    next_token_id = torch.argmax(last_logits, dim=-1).item()

                    # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸtoken
                    if next_token_id == 2:  # </s> token
                        break

                    generated_tokens.append(next_token_id)

                    # åˆ›å»ºæ–°çš„tokenå¹¶æ‹¼æ¥åˆ°åºåˆ—
                    next_token_tensor = torch.tensor([[next_token_id]], dtype=torch.long, device=self.device)
                    current_tokens = torch.cat([current_tokens, next_token_tensor], dim=1)

                except Exception as e:
                    break

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        if generated_tokens:
            generated_text = self.decode(generated_tokens)
            return generated_text.strip()
        else:
            return ""

    def extract_answer(self, response: str) -> str:
        """
        ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„å›å¤

        Returns:
            æå–çš„ç­”æ¡ˆï¼ˆ1-4çš„æ•°å­—ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # æ¸…ç†å“åº”æ–‡æœ¬
        response = response.strip()

        # å°è¯•å¤šç§æ¨¡å¼æå–ç­”æ¡ˆ
        patterns = [
            r'\b([1-4])\b',  # åŒ¹é…å•ç‹¬çš„æ•°å­—1-4
            r'ç­”æ¡ˆ[æ˜¯ä¸º]?\s*([1-4])',  # åŒ¹é…"ç­”æ¡ˆæ˜¯X"
            r'é€‰æ‹©\s*([1-4])',  # åŒ¹é…"é€‰æ‹©X"
            r'([1-4])\s*[.ã€‚]',  # åŒ¹é…"X."
            r'é€‰é¡¹\s*([1-4])',  # åŒ¹é…"é€‰é¡¹X"
        ]

        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1)

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›å“åº”çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚æœæ˜¯1-4ï¼‰
        if len(response) > 0 and response[0] in '1234':
            return response[0]

        return ""

    def load_dataset(self, data_file: str) -> List[Dict]:
        """
        åŠ è½½æ•°æ®é›†

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            æ•°æ®é›†åˆ—è¡¨
        """
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)

            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {data_file}")
            print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} æ¡")
            return dataset

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []

    def calculate_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            ground_truths: çœŸå®æ ‡ç­¾åˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        # è¿‡æ»¤æ‰ç©ºé¢„æµ‹
        filtered_predictions = []
        filtered_ground_truths = []

        for pred, truth in zip(predictions, ground_truths):
            if pred:  # åªè€ƒè™‘æœ‰é¢„æµ‹ç»“æœçš„æ ·æœ¬
                filtered_predictions.append(pred)
                filtered_ground_truths.append(truth)

        if not filtered_predictions:
            return {}

        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(filtered_ground_truths, filtered_predictions)

        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        precision, recall, f1, support = precision_recall_fscore_support(
            filtered_ground_truths, filtered_predictions, average='macro', zero_division=0
        )

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_macro': f1,
            'total_samples': len(predictions),
            'answered_samples': len(filtered_predictions),
            'answer_extraction_rate': len(filtered_predictions) / len(predictions) if predictions else 0
        }

    def evaluate_dataset(self, data_file: str, dataset_tag: str, output_dir: str) -> Dict:
        """
        è¯„ä¼°æ•°æ®é›†

        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_tag: æ•°æ®é›†æ ‡ç­¾
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(data_file)
        if not dataset:
            return {}

        # å‡†å¤‡ç»“æœå­˜å‚¨
        results = []
        predictions = []
        ground_truths = []

        print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {dataset_tag} æ•°æ®é›†...")
        print("=" * 60)

        # æ‰¹é‡å¤„ç†æ•°æ®
        for i, item in enumerate(tqdm(dataset, desc="è¯„ä¼°è¿›åº¦")):
            instruction = item.get('instruction', '')
            expected_output = item.get('output', '').strip()

            if not instruction:
                print(f"âš ï¸  ç¬¬ {i+1} æ¡æ•°æ®ç¼ºå°‘instructionå­—æ®µï¼Œè·³è¿‡")
                continue

            # ç”Ÿæˆæ¨¡å‹å›å¤
            model_response = self.generate_response(instruction)

            # æå–ç­”æ¡ˆ
            extracted_answer = self.extract_answer(model_response)

            # åˆ¤æ–­æ˜¯å¦ä¸€è‡´
            is_correct = extracted_answer == expected_output

            # æ‰“å°å‰ä¸‰æ¡æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            if i < 3:
                print(f"\n=== æ ·æœ¬ {i+1} ===")
                print(f"é—®é¢˜ (instruction): {instruction}")
                print(f"æœŸæœ›ç­”æ¡ˆ (output): {expected_output}")
                print(f"æ¨¡å‹ç”Ÿæˆå›ç­”: {model_response}")
                print(f"æå–ç­”æ¡ˆ: {extracted_answer}")
                print(f"æ˜¯å¦æ­£ç¡®: {is_correct}")
                print("=" * 60)

            # è®°å½•ç»“æœ
            result_item = {
                "question_id": i + 1,
                "instruction": instruction,
                "expected_answer": expected_output,
                "model_response": model_response,
                "extracted_answer": extracted_answer,
                "is_correct": is_correct
            }

            results.append(result_item)
            predictions.append(extracted_answer)
            ground_truths.append(expected_output)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = self.calculate_metrics(predictions, ground_truths)

        # ç»„ç»‡æœ€ç»ˆç»“æœ
        final_results = {
            "dataset_info": {
                "dataset_tag": dataset_tag,
                "data_file": data_file,
                "total_questions": len(dataset),
                "answered_questions": metrics.get('answered_samples', 0),
            },
            "performance_metrics": {
                "accuracy": metrics.get('accuracy', 0),
                "precision": metrics.get('precision', 0),
                "recall": metrics.get('recall', 0),
                "f1_macro": metrics.get('f1_macro', 0),
                "answer_extraction_rate": metrics.get('answer_extraction_rate', 0),
            },
            "statistics": {
                "overall_accuracy": metrics.get('accuracy', 0),
                "answer_extraction_rate": metrics.get('answer_extraction_rate', 0),
            },
            "timestamp": datetime.now().isoformat()
        }

        # ä¿å­˜è¯¦ç»†ç»“æœ
        answers_file = os.path.join(output_dir, "generated_answers.json")
        with open(answers_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = os.path.join(output_dir, "eval_results.json")
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ•´ä½“å‡†ç¡®ç‡: {final_results['performance_metrics']['accuracy']:.4f}")
        print(f"ğŸ“Š ç­”æ¡ˆæå–ç‡: {final_results['performance_metrics']['answer_extraction_rate']:.4f}")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

        return final_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CultureBankæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--dataset_id", type=int, required=True,
                       help="æ•°æ®é›†ID (2=CulturalBench, 3=normad, 4=cultureLLM, 5=cultureAtlas)")
    parser.add_argument("--data_file", type=str, required=True,
                       help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset_tag", type=str, required=True,
                       help="æ•°æ®é›†æ ‡ç­¾")
    parser.add_argument("--output_dir", type=str, default="./",
                       help="è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    print("ğŸ›ï¸  CultureBankæ¨¡å‹è¯„ä¼°å™¨")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset_tag} (ID: {args.dataset_id})")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {args.data_file}")
    print("=" * 60)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CultureBankEvaluator()

    # åŠ è½½æ¨¡å‹
    if not evaluator.load_model():
        print("âŒ CultureBankæ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # æ‰§è¡Œè¯„ä¼°
    evaluator.evaluate_dataset(args.data_file, args.dataset_tag, args.output_dir)

    print("\nğŸ‰ è¯„ä¼°ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()