#!/usr/bin/env python3
"""
CultureBankæ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºåŠ è½½å®Œæ•´çš„CultureBankæ¨¡å‹å¹¶è¿›è¡Œå¯¹è¯æµ‹è¯•

ä¾èµ–å®‰è£…:
pip install transformers peft torch accelerate bitsandbytes

ä½¿ç”¨æ–¹æ³•:
python eval_culturebank.py
"""

import os
import time
import torch
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, AutoPeftModelForCausalLM
import warnings
warnings.filterwarnings("ignore")


class CultureBankEvaluator:
    def __init__(self, model_path="./sft_preference_v0.3", use_4bit=True):
        """
        åˆå§‹åŒ–CultureBankè¯„ä¼°å™¨

        Args:
            model_path: LoRA adapterè·¯å¾„
            use_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
        """
        self.model_path = model_path
        self.base_model_name = "meta-llama/Llama-2-7b-chat-hf"
        self.use_4bit = use_4bit
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ”§ è®¾å¤‡ä¿¡æ¯: {self.device}")
        print(f"ğŸ”§ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ”§ GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ”§ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        cpu_memory = memory_info.rss / 1024**3  # GB

        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return f"CPU: {cpu_memory:.1f}GB, GPU: {gpu_memory:.1f}GB/{gpu_total:.1f}GB"
        else:
            return f"CPU: {cpu_memory:.1f}GB"

    def load_model_method1(self):
        """
        æ–¹æ³•1: ä½¿ç”¨AutoPeftModelForCausalLMç›´æ¥åŠ è½½
        """
        print("\nğŸš€ æ–¹æ³•1: ä½¿ç”¨AutoPeftModelForCausalLMåŠ è½½...")

        try:
            # é…ç½®é‡åŒ–å‚æ•°
            if self.use_4bit:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                print("âœ… å¯ç”¨4bité‡åŒ–")
            else:
                bnb_config = None

            # åŠ è½½æ¨¡å‹
            self.model = AutoPeftModelForCausalLM.from_pretrained(
                self.model_path,
                quantization_config=bnb_config,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("âœ… æ–¹æ³•1åŠ è½½æˆåŠŸ!")
            print(f"ğŸ“Š å†…å­˜ä½¿ç”¨: {self.get_memory_usage()}")
            return True

        except Exception as e:
            print(f"âŒ æ–¹æ³•1å¤±è´¥: {str(e)}")
            return False

    def load_model_method2(self):
        """
        æ–¹æ³•2: å…ˆåŠ è½½åŸºåº§æ¨¡å‹ï¼Œå†åŠ è½½LoRA adapter
        """
        print("\nğŸš€ æ–¹æ³•2: åˆ†æ­¥åŠ è½½åŸºåº§æ¨¡å‹+LoRA...")

        try:
            # é…ç½®é‡åŒ–å‚æ•°
            if self.use_4bit:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                print("âœ… å¯ç”¨4bité‡åŒ–")
            else:
                bnb_config = None

            # åŠ è½½åŸºåº§æ¨¡å‹
            print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {self.base_model_name}")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                quantization_config=bnb_config,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )

            # åŠ è½½LoRA adapter
            print(f"ğŸ“¥ åŠ è½½LoRA adapter: {self.model_path}")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.model_path,
                torch_dtype=torch.bfloat16
            )

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("âœ… æ–¹æ³•2åŠ è½½æˆåŠŸ!")
            print(f"ğŸ“Š å†…å­˜ä½¿ç”¨: {self.get_memory_usage()}")
            return True

        except Exception as e:
            print(f"âŒ æ–¹æ³•2å¤±è´¥: {str(e)}")
            return False

    def load_model(self):
        """
        å°è¯•åŠ è½½æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æ–¹æ³•1ï¼Œå¤±è´¥åå°è¯•æ–¹æ³•2
        """
        print("=" * 60)
        print("ğŸ¯ å¼€å§‹åŠ è½½CultureBankæ¨¡å‹...")
        print("=" * 60)

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            return False

        # å°è¯•æ–¹æ³•1
        if self.load_model_method1():
            return True

        # æ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2
        print("\nğŸ”„ æ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2...")
        if self.load_model_method2():
            return True

        print("âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†!")
        return False

    def format_chat_prompt(self, message, system_message=None):
        """
        æ ¼å¼åŒ–Llama-2 chatæ ¼å¼çš„æç¤º
        """
        if system_message is None:
            system_message = "You are a helpful, respectful and honest assistant with cultural knowledge."

        # Llama-2 chatæ ¼å¼
        prompt = f"<s>[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{message} [/INST]"
        return prompt

    def generate_response(self, message, max_length=512, temperature=0.7, do_sample=True):
        """
        ç”Ÿæˆæ¨¡å‹å“åº”

        Args:
            message: è¾“å…¥æ¶ˆæ¯
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
        """
        if self.model is None or self.tokenizer is None:
            return "âŒ æ¨¡å‹æœªåŠ è½½"

        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_chat_prompt(message)
        print(f"ğŸ”¤ æ ¼å¼åŒ–æç¤º: {prompt[:100]}...")

        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        inputs = inputs.to(self.device)

        print(f"ğŸ“ è¾“å…¥tokensé•¿åº¦: {inputs.shape[1]}")

        # ç”Ÿæˆå“åº”
        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=do_sample,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        end_time = time.time()

        # è§£ç å“åº”
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥æç¤ºï¼‰
        response = full_response[len(prompt):].strip()

        # æ€§èƒ½ç»Ÿè®¡
        generation_time = end_time - start_time
        tokens_generated = outputs.shape[1] - inputs.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0

        print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
        print(f"ğŸ¯ ç”Ÿæˆtokens: {tokens_generated}")
        print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
        print(f"ğŸ“Š å†…å­˜ä½¿ç”¨: {self.get_memory_usage()}")

        return response

    def run_evaluation(self):
        """
        è¿è¡Œè¯„ä¼°æµ‹è¯•
        """
        # åŠ è½½æ¨¡å‹
        if not self.load_model():
            return

        print("\n" + "=" * 60)
        print("ğŸ‰ CultureBankæ¨¡å‹åŠ è½½æˆåŠŸï¼å¼€å§‹è¯„ä¼°...")
        print("=" * 60)

        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            "ä½ å¥½",
            "ä»‹ç»ä¸€ä¸‹ä¸­å›½çš„ä¼ ç»ŸèŠ‚æ—¥",
            "What is the significance of the Spring Festival in Chinese culture?",
            "è¯·è§£é‡Šä¸€ä¸‹å„’å®¶æ€æƒ³çš„æ ¸å¿ƒç†å¿µ",
            "Tell me about traditional Chinese medicine"
        ]

        for i, test_input in enumerate(test_cases, 1):
            print(f"\n{'='*20} æµ‹è¯• {i}/{len(test_cases)} {'='*20}")
            print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {test_input}")
            print("ğŸ¤– CultureBankå›åº”:")
            print("-" * 50)

            response = self.generate_response(test_input)
            print(response)
            print("-" * 50)

            # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…GPUè¿‡çƒ­
            time.sleep(1)

        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼æœ€ç»ˆå†…å­˜ä½¿ç”¨: {self.get_memory_usage()}")

    def interactive_chat(self):
        """
        äº¤äº’å¼å¯¹è¯æ¨¡å¼
        """
        if self.model is None or self.tokenizer is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œäº¤äº’")
            return

        print("\n" + "=" * 60)
        print("ğŸ’¬ è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        print("=" * 60)

        while True:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()

            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            print("ğŸ¤– CultureBank:")
            print("-" * 40)
            response = self.generate_response(user_input)
            print(response)
            print("-" * 40)


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ›ï¸  CultureBankæ¨¡å‹è¯„ä¼°å™¨")
    print("åŸºäºLlama-2-7b-chat-hf + LoRAå¾®è°ƒ")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    try:
        import transformers
        import peft
        import torch
        print(f"âœ… transformersç‰ˆæœ¬: {transformers.__version__}")
        print(f"âœ… peftç‰ˆæœ¬: {peft.__version__}")
        print(f"âœ… torchç‰ˆæœ¬: {torch.__version__}")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install transformers peft torch accelerate bitsandbytes")
        return

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CultureBankEvaluator()

    # è¿è¡Œè‡ªåŠ¨è¯„ä¼°
    evaluator.run_evaluation()

    # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
    while True:
        choice = input("\nğŸ¤” æ˜¯å¦è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼? (y/n): ").strip().lower()
        if choice in ['y', 'yes', 'æ˜¯']:
            evaluator.interactive_chat()
            break
        elif choice in ['n', 'no', 'å¦']:
            print("ğŸ‘‹ è¯„ä¼°å®Œæˆï¼Œå†è§ï¼")
            break
        else:
            print("è¯·è¾“å…¥ y æˆ– n")


if __name__ == "__main__":
    main()