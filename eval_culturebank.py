#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆCultureBankæ¨¡å‹åŠ è½½è„šæœ¬
åŠ è½½å®Œæ•´çš„CultureBankæ¨¡å‹å¹¶ç”Ÿæˆå›åº”
"""

import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

class CultureBankEvaluator:
    def __init__(self, model_path="./CultureBank-Llama2-SFT/sft_preference_v0.3", use_4bit=True, local_base_model=None):
        """
        åˆå§‹åŒ–CultureBankè¯„ä¼°å™¨

        Args:
            model_path: LoRA adapterè·¯å¾„
            use_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
            local_base_model: æœ¬åœ°åŸºåº§æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨HF Hubæ¨¡å‹
        """
        self.model_path = model_path

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        if local_base_model and os.path.exists(local_base_model):
            self.base_model_name = local_base_model
            print(f"ğŸ  ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹: {local_base_model}")
        else:
            # ä½¿ç”¨HFæ ¼å¼çš„Llama-2-7b-chatæ¨¡å‹
            self.base_model_name = "NousResearch/Llama-2-7b-chat-hf"  # æˆ– "meta-llama/Llama-2-7b-chat-hf"
            print(f"ğŸŒ ä½¿ç”¨HF Hubæ¨¡å‹: {self.base_model_name}")

        self.use_4bit = use_4bit
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def load_model(self):
        """
        åŠ è½½CultureBankæ¨¡å‹ (åŸºåº§+LoRA adapter)
        """
        print("ğŸ¯ å¼€å§‹åŠ è½½CultureBankæ¨¡å‹...")

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            return False

        try:
            # é…ç½®é‡åŒ–å‚æ•°
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=self.use_4bit,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
            ) if self.use_4bit else None

            # åŠ è½½åŸºåº§æ¨¡å‹
            print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {self.base_model_name}")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                quantization_config=bnb_config,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )

            # åŠ è½½LoRA adapter
            print(f"ğŸ“¥ åŠ è½½LoRA adapter: {self.model_path}")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.model_path,
                torch_dtype=torch.bfloat16
            )

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def format_chat_prompt(self, message, system_message=None):
        """
        æ ¼å¼åŒ–Llama-2 chatæ ¼å¼çš„æç¤º
        """
        if system_message is None:
            system_message = "You are a helpful, respectful and honest assistant with cultural knowledge."

        # Llama-2 chatæ ¼å¼
        prompt = f"<s>[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{message} [/INST]"
        return prompt

    def generate_response(self, message, max_length=512, temperature=0.7, do_sample=True):
        """
        ç”Ÿæˆæ¨¡å‹å“åº”

        Args:
            message: è¾“å…¥æ¶ˆæ¯
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
        """
        if self.model is None or self.tokenizer is None:
            return "âŒ æ¨¡å‹æœªåŠ è½½"

        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self.format_chat_prompt(message)
        print(f"ğŸ”¤ æ ¼å¼åŒ–æç¤º: {prompt[:100]}...")

        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        inputs = inputs.to(self.device)

        print(f"ğŸ“ è¾“å…¥tokensé•¿åº¦: {inputs.shape[1]}")

        # ç”Ÿæˆå“åº”
        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=do_sample,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        end_time = time.time()

        # è§£ç å“åº”
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥æç¤ºï¼‰
        response = full_response[len(prompt):].strip()

        # æ€§èƒ½ç»Ÿè®¡
        generation_time = end_time - start_time
        tokens_generated = outputs.shape[1] - inputs.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0

        print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
        print(f"ğŸ¯ ç”Ÿæˆtokens: {tokens_generated}")
        print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")

        return response

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ›ï¸  CultureBankæ¨¡å‹ç®€åŒ–ç‰ˆåŠ è½½")
    print("=" * 60)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    # å¦‚æœä½ æƒ³ä½¿ç”¨æœ¬åœ°Metaæ ¼å¼æ¨¡å‹ï¼Œå–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šï¼š
    # evaluator = CultureBankEvaluator(local_base_model="./Llama-2-7b-chat")
    evaluator = CultureBankEvaluator()  # ä½¿ç”¨HF Hubæ¨¡å‹

    # åŠ è½½æ¨¡å‹
    if not evaluator.load_model():
        return

    # æµ‹è¯•ç”¨ä¾‹
    test_input = "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä¸­å›½çš„ä¼ ç»ŸèŠ‚æ—¥"
    print(f"\nğŸ‘¤ ç”¨æˆ·è¾“å…¥: {test_input}")
    print("ğŸ¤– æ¨¡å‹å›åº”:")
    response = evaluator.generate_response(test_input)
    print(response)

if __name__ == "__main__":
    main()
