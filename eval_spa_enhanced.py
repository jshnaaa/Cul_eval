#!/usr/bin/env python3
"""
CultureSPAæ¨¡å‹å¢å¼ºè¯„ä¼°è„šæœ¬
æ”¯æŒé…ç½®æ–‡ä»¶ã€æ–­ç‚¹ç»­ä¼ ã€è¯¦ç»†æ—¥å¿—ç­‰é«˜çº§åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
python eval_spa_enhanced.py --dataset_id 2 --config eval_config.json

ä¾èµ–:
pip install transformers torch tqdm scikit-learn
"""

import json
import argparse
import re
import time
import os
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import warnings
warnings.filterwarnings("ignore")


class EnhancedCultureSPAEvaluator:
    def __init__(self, config_file: str = "eval_config.json"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆCultureSPAè¯„ä¼°å™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self.load_config(config_file)
        self.model = None
        self.tokenizer = None
        self.logger = self.setup_logger()

        # ä»é…ç½®è·å–è®¾å¤‡ä¿¡æ¯
        device_config = self.config["model_settings"]["device"]
        self.device = device_config if device_config != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")

        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            return config
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_file}: {e}")
            print("ä½¿ç”¨é»˜è®¤é…ç½®...")
            return self.get_default_config()

    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "model_settings": {
                "default_model_path": None,
                "generation_params": {
                    "max_length": 512,
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "repetition_penalty": 1.1
                },
                "device": "auto"
            },
            "evaluation_settings": {
                "output_dir": "./eval_results",
                "save_detailed_results": True,
                "save_evaluation_metrics": True,
                "progress_update_interval": 10,
                "answer_extraction_patterns": [
                    "\\b([1-4])\\b",
                    "ç­”æ¡ˆ[æ˜¯ä¸º]?\\s*([1-4])",
                    "é€‰æ‹©\\s*([1-4])",
                    "([1-4])\\s*[.ã€‚]",
                    "é€‰é¡¹\\s*([1-4])"
                ]
            },
            "logging": {
                "enable_detailed_logging": True,
                "log_model_responses": True,
                "log_extraction_failures": True
            }
        }

    def setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('CultureSPA_Evaluator')
        logger.setLevel(logging.INFO)

        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)

        logger.addHandler(console_handler)
        return logger

    def load_model(self, model_path: Optional[str] = None):
        """åŠ è½½CultureSPAæ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            # ç¡®å®šæ¨¡å‹è·¯å¾„
            if model_path is None:
                model_path = self.config["model_settings"]["default_model_path"]

            if model_path is None:
                model_path = os.environ.get('CULTURESPA_MODEL_PATH', './culturespa_model')

            self.logger.info("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

            self.logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.logger.info("æ¨¡å‹å·²é€šè¿‡ device_map='auto' åŠ è½½ã€‚")
            self.logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
            return True

        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def extract_answer(self, response: str) -> str:
        """
        ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„å›å¤

        Returns:
            æå–çš„ç­”æ¡ˆï¼ˆ1-4çš„æ•°å­—ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        response = response.strip()

        # ä»é…ç½®è·å–æå–æ¨¡å¼
        patterns = self.config["evaluation_settings"]["answer_extraction_patterns"]

        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1)

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›å“åº”çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚æœæ˜¯1-4ï¼‰
        if len(response) > 0 and response[0] in '1234':
            return response[0]

        # è®°å½•æå–å¤±è´¥
        if self.config["logging"]["log_extraction_failures"]:
            self.logger.warning(f"ç­”æ¡ˆæå–å¤±è´¥: {response[:100]}...")

        return ""

    def generate_response(self, instruction: str) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å›å¤

        Args:
            instruction: è¾“å…¥æŒ‡ä»¤

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„å›å¤
        """
        try:
            # è·å–ç”Ÿæˆå‚æ•°
            gen_params = self.config["model_settings"]["generation_params"]

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(instruction, return_tensors="pt")
            input_length = inputs.shape[1]

            if self.config["logging"]["log_model_responses"]:
                self.logger.debug(f"ç”Ÿæˆå›å¤ (è¾“å…¥é•¿åº¦: {input_length})")
                self.logger.debug(f"ç”¨æˆ·é—®é¢˜: {instruction[:200]}...")

            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.to(self.device),
                    max_length=min(input_length + gen_params["max_length"], 2048),
                    temperature=gen_params["temperature"],
                    do_sample=gen_params["temperature"] > 0,
                    top_p=gen_params.get("top_p", 0.9),
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=gen_params.get("repetition_penalty", 1.1),
                )

            # è§£ç å›å¤ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = full_response[len(instruction):].strip()

            if self.config["logging"]["log_model_responses"]:
                self.logger.debug(f"æ¨¡å‹å›å¤: {response[:200]}...")

            return response

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
            return ""

    def load_dataset(self, data_file: str) -> List[Dict]:
        """
        åŠ è½½æ•°æ®é›†

        Args:
            data_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„

        Returns:
            æ•°æ®é›†åˆ—è¡¨
        """
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {data_file}")
            self.logger.info(f"æ•°æ®é›†å¤§å°: {len(data)} æ¡")
            return data

        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []

    def save_checkpoint(self, results: List[Dict], checkpoint_file: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = {
                "timestamp": datetime.now().isoformat(),
                "processed_count": len(results),
                "results": results
            }
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")

    def load_checkpoint(self, checkpoint_file: str) -> Tuple[List[Dict], int]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            if os.path.exists(checkpoint_file):
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                results = checkpoint_data.get("results", [])
                processed_count = checkpoint_data.get("processed_count", 0)
                self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²å¤„ç† {processed_count} æ¡æ•°æ®")
                return results, processed_count
        except Exception as e:
            self.logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")

        return [], 0

    def evaluate_dataset_with_resume(self, data_file: str, dataset_tag: str, output_dir: str = "./") -> Dict:
        """
        æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ•°æ®é›†è¯„ä¼°

        Args:
            data_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            dataset_tag: æ•°æ®é›†æ ‡ç­¾
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(data_file)
        if not dataset:
            return {}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_file = os.path.join(output_dir, f"checkpoint_{dataset_tag}.json")

        # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
        results, start_index = self.load_checkpoint(checkpoint_file)

        # å‡†å¤‡ç»“æœå­˜å‚¨
        predictions = []
        ground_truths = []

        # è¯„ä¼°å¼€å§‹æ—¶é—´
        start_time = time.time()

        self.logger.info(f"å¼€å§‹è¯„ä¼° {dataset_tag} æ•°æ®é›†...")
        if start_index > 0:
            self.logger.info(f"ä»ç¬¬ {start_index + 1} æ¡æ•°æ®ç»§ç»­è¯„ä¼°")

        # è¿›åº¦æ›´æ–°é—´éš”
        progress_interval = self.config["evaluation_settings"]["progress_update_interval"]

        # æ‰¹é‡å¤„ç†æ•°æ®
        for i in range(start_index, len(dataset)):
            item = dataset[i]
            instruction = item.get('instruction', '')
            expected_output = item.get('output', '').strip()

            if not instruction:
                self.logger.warning(f"ç¬¬ {i+1} æ¡æ•°æ®ç¼ºå°‘instructionå­—æ®µï¼Œè·³è¿‡")
                continue

            # ç”Ÿæˆæ¨¡å‹å›å¤
            model_response = self.generate_response(instruction)

            # æå–ç­”æ¡ˆ
            extracted_answer = self.extract_answer(model_response)

            # åˆ¤æ–­æ˜¯å¦ä¸€è‡´
            is_correct = extracted_answer == expected_output

            # è®°å½•ç»“æœ
            result_item = {
                "question_id": i + 1,
                "instruction": instruction,
                "expected_answer": expected_output,
                "model_response": model_response,
                "extracted_answer": extracted_answer,
                "is_correct": is_correct,
                "timestamp": datetime.now().isoformat()
            }

            results.append(result_item)

            # ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼ˆåªæœ‰æˆåŠŸæå–ç­”æ¡ˆçš„æ‰å‚ä¸è®¡ç®—ï¼‰
            if extracted_answer:
                predictions.append(extracted_answer)
                ground_truths.append(expected_output)

            # å®šæœŸæ˜¾ç¤ºè¿›åº¦å’Œä¿å­˜æ£€æŸ¥ç‚¹
            if (i + 1) % progress_interval == 0:
                current_accuracy = sum(1 for r in results if r['is_correct']) / len(results)
                self.logger.info(f"å·²å¤„ç† {i+1}/{len(dataset)} æ¡ï¼Œå½“å‰å‡†ç¡®ç‡: {current_accuracy:.3f}")

                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(results, checkpoint_file)

        # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆè¯„ä¼°å®Œæˆï¼‰
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
            self.logger.info("è¯„ä¼°å®Œæˆï¼Œåˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶")

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        evaluation_metrics = self.calculate_metrics(predictions, ground_truths)

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_questions = len(dataset)
        answered_questions = len([r for r in results if r['extracted_answer']])
        correct_answers = len([r for r in results if r['is_correct']])

        # è¯„ä¼°ç»“æŸæ—¶é—´
        end_time = time.time()
        evaluation_time = end_time - start_time

        # å‡†å¤‡æœ€ç»ˆç»“æœ
        final_results = {
            "dataset_info": {
                "dataset_tag": dataset_tag,
                "data_file": data_file,
                "total_questions": total_questions,
                "answered_questions": answered_questions,
                "unanswered_questions": total_questions - answered_questions,
            },
            "performance_metrics": evaluation_metrics,
            "statistics": {
                "overall_accuracy": correct_answers / total_questions if total_questions > 0 else 0,
                "answer_extraction_rate": answered_questions / total_questions if total_questions > 0 else 0,
                "evaluation_time_seconds": evaluation_time,
                "questions_per_second": total_questions / evaluation_time if evaluation_time > 0 else 0,
            },
            "config_used": self.config,
            "timestamp": datetime.now().isoformat()
        }

        # ä¿å­˜ç»“æœ
        self.save_results(results, final_results, dataset_tag, output_dir)

        return final_results

    def calculate_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not predictions or not ground_truths:
            return {
                "accuracy": 0.0,
                "precision_macro": 0.0,
                "recall_macro": 0.0,
                "f1_macro": 0.0,
                "precision_micro": 0.0,
                "recall_micro": 0.0,
                "f1_micro": 0.0,
                "per_class_metrics": {}
            }

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(ground_truths, predictions)

        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            ground_truths, predictions, average='macro', zero_division=0
        )

        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
            ground_truths, predictions, average='micro', zero_division=0
        )

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
            ground_truths, predictions, average=None, zero_division=0
        )

        # è·å–æ‰€æœ‰ç±»åˆ«
        unique_labels = sorted(list(set(ground_truths + predictions)))

        per_class_metrics = {}
        for i, label in enumerate(unique_labels):
            if i < len(precision_per_class):
                per_class_metrics[label] = {
                    "precision": float(precision_per_class[i]),
                    "recall": float(recall_per_class[i]),
                    "f1": float(f1_per_class[i]),
                    "support": int(support[i])
                }

        return {
            "accuracy": float(accuracy),
            "precision_macro": float(precision_macro),
            "recall_macro": float(recall_macro),
            "f1_macro": float(f1_macro),
            "precision_micro": float(precision_micro),
            "recall_micro": float(recall_micro),
            "f1_micro": float(f1_micro),
            "per_class_metrics": per_class_metrics
        }

    def save_results(self, results: List[Dict], final_results: Dict, dataset_tag: str, output_dir: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if self.config["evaluation_settings"]["save_detailed_results"]:
            answers_file = os.path.join(output_dir, f"generated_answers_{dataset_tag}.json")
            with open(answers_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            self.logger.info(f"è¯¦ç»†ç­”æ¡ˆå·²ä¿å­˜: {answers_file}")

        # ä¿å­˜è¯„ä¼°ç»“æœ
        if self.config["evaluation_settings"]["save_evaluation_metrics"]:
            eval_file = os.path.join(output_dir, f"eval_result_{dataset_tag}.json")
            with open(eval_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            self.logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_file}")

        # æ‰“å°ç»“æœæ‘˜è¦
        self.print_evaluation_summary(final_results)

    def print_evaluation_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ‰ è¯„ä¼°å®Œæˆï¼ç»“æœæ‘˜è¦:")
        print("=" * 60)

        dataset_info = results["dataset_info"]
        metrics = results["performance_metrics"]
        stats = results["statistics"]

        print(f"ğŸ“Š æ•°æ®é›†: {dataset_info['dataset_tag']}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {dataset_info['data_file']}")
        print(f"ğŸ”¢ æ€»é—®é¢˜æ•°: {dataset_info['total_questions']}")
        print(f"âœ… æˆåŠŸå›ç­”: {dataset_info['answered_questions']}")
        print(f"âŒ æœªèƒ½å›ç­”: {dataset_info['unanswered_questions']}")

        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æ•´ä½“å‡†ç¡®ç‡: {stats['overall_accuracy']:.4f}")
        print(f"  ç­”æ¡ˆæå–ç‡: {stats['answer_extraction_rate']:.4f}")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision-Macro): {metrics['precision_macro']:.4f}")
        print(f"  å¬å›ç‡ (Recall-Macro): {metrics['recall_macro']:.4f}")
        print(f"  F1åˆ†æ•° (F1-Macro): {metrics['f1_macro']:.4f}")

        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"  è¯„ä¼°æ€»æ—¶é—´: {stats['evaluation_time_seconds']:.2f} ç§’")
        print(f"  å¤„ç†é€Ÿåº¦: {stats['questions_per_second']:.2f} é—®é¢˜/ç§’")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CultureSPAæ¨¡å‹å¢å¼ºè¯„ä¼°è„šæœ¬")
    parser.add_argument("--dataset_id", type=int, required=True,
                       help="æ•°æ®é›†ID")
    parser.add_argument("--config", type=str, default="eval_config.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_path", type=str, default=None,
                       help="æ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰")

    args = parser.parse_args()

    print("ğŸ›ï¸  CultureSPAæ¨¡å‹å¢å¼ºè¯„ä¼°å™¨")
    print(f"ğŸ“Š æ•°æ®é›†ID: {args.dataset_id}")
    print("=" * 60)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = EnhancedCultureSPAEvaluator(config_file=args.config)

    # ä»é…ç½®è·å–æ•°æ®é›†ä¿¡æ¯
    datasets_config = evaluator.config.get("datasets", {})
    dataset_id_str = str(args.dataset_id)

    if dataset_id_str not in datasets_config:
        print(f"âŒ æ— æ•ˆçš„æ•°æ®é›†ID: {args.dataset_id}")
        print(f"æ”¯æŒçš„æ•°æ®é›†: {list(datasets_config.keys())}")
        return

    dataset_info = datasets_config[dataset_id_str]
    data_file = dataset_info["file_path"]
    dataset_tag = dataset_info["name"]

    print(f"ğŸ“Š æ•°æ®é›†: {dataset_tag}")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return

    # ç¡®å®šè¾“å‡ºç›®å½•
    output_dir = args.output_dir or evaluator.config["evaluation_settings"]["output_dir"]
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    if not evaluator.load_model(args.model_path):
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè¯„ä¼°")
        return

    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset_with_resume(
        data_file=data_file,
        dataset_tag=dataset_tag,
        output_dir=output_dir
    )

    if results:
        print("âœ… è¯„ä¼°æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ è¯„ä¼°å¤±è´¥ï¼")


if __name__ == "__main__":
    main()