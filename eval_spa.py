#!/usr/bin/env python3
"""
CultureSPAæ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒå¯¹å¤šä¸ªæ–‡åŒ–çŸ¥è¯†æ•°æ®é›†è¿›è¡Œæ‰¹é‡è¯„ä¼°

ä½¿ç”¨æ–¹æ³•:
python eval_spa.py --dataset_id 2 --data_file /path/to/dataset.json --dataset_tag CulturalBench

ä¾èµ–:
pip install transformers torch tqdm scikit-learn
"""

import json
import argparse
import re
import time
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import warnings
warnings.filterwarnings("ignore")


class CultureSPAEvaluator:
    def __init__(self, model_path: str = None, device: str = "auto"):
        """
        åˆå§‹åŒ–CultureSPAè¯„ä¼°å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
            device: è®¾å¤‡è®¾ç½®
        """
        self.model_path = model_path
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None

        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_model(self):
        """åŠ è½½CultureSPAæ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
            # æ ¹æ®å®é™…æ¨¡å‹è·¯å¾„è°ƒæ•´ï¼Œè¿™é‡Œä½¿ç”¨é€šç”¨åŠ è½½æ–¹å¼
            if self.model_path:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤ä½ç½®åŠ è½½
                model_path = os.environ.get('CULTURESPA_MODEL_PATH', './culturespa_model')
                self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                self.model_path = model_path

            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                dtype=torch.float16,  # ä½¿ç”¨dtypeæ›¿ä»£torch_dtype
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print("æ¨¡å‹å·²é€šè¿‡ device_map='auto' åŠ è½½ã€‚")
            print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def extract_answer(self, response: str) -> str:
        """
        ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„å›å¤

        Returns:
            æå–çš„ç­”æ¡ˆï¼ˆ1-4çš„æ•°å­—ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # æ¸…ç†å“åº”æ–‡æœ¬
        response = response.strip()

        # å°è¯•å¤šç§æ¨¡å¼æå–ç­”æ¡ˆ
        patterns = [
            r'\b([1-4])\b',  # åŒ¹é…å•ç‹¬çš„æ•°å­—1-4
            r'ç­”æ¡ˆ[æ˜¯ä¸º]?\s*([1-4])',  # åŒ¹é…"ç­”æ¡ˆæ˜¯X"
            r'é€‰æ‹©\s*([1-4])',  # åŒ¹é…"é€‰æ‹©X"
            r'([1-4])\s*[.ã€‚]',  # åŒ¹é…"X."
            r'é€‰é¡¹\s*([1-4])',  # åŒ¹é…"é€‰é¡¹X"
        ]

        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1)

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›å“åº”çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚æœæ˜¯1-4ï¼‰
        if len(response) > 0 and response[0] in '1234':
            return response[0]

        return ""

    def generate_response(self, instruction: str, max_length: int = 512, temperature: float = 0.1) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å›å¤

        Args:
            instruction: è¾“å…¥æŒ‡ä»¤
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„å›å¤
        """
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(instruction, return_tensors="pt")
            input_length = inputs.shape[1]

            print(f"ç”Ÿæˆå›å¤ (è¾“å…¥é•¿åº¦: {input_length})...")
            print(f"ç³»ç»ŸæŒ‡ä»¤: ")
            print(f"ç”¨æˆ·é—®é¢˜: {repr((instruction,))}")

            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.to(self.device),
                    max_length=min(input_length + max_length, 2048),
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.9 if temperature > 0 else None,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                )

            # è§£ç å›å¤ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = full_response[len(instruction):].strip()

            print(f"æ¨¡å‹å›å¤: {response}")
            return response

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
            return ""

    def load_dataset(self, data_file: str) -> List[Dict]:
        """
        åŠ è½½æ•°æ®é›†

        Args:
            data_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„

        Returns:
            æ•°æ®é›†åˆ—è¡¨
        """
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {data_file}")
            print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(data)} æ¡")
            return data

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []

    def evaluate_dataset(self, data_file: str, dataset_tag: str, output_dir: str = "./") -> Dict:
        """
        è¯„ä¼°æ•´ä¸ªæ•°æ®é›†

        Args:
            data_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            dataset_tag: æ•°æ®é›†æ ‡ç­¾
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset(data_file)
        if not dataset:
            return {}

        # å‡†å¤‡ç»“æœå­˜å‚¨
        results = []
        predictions = []
        ground_truths = []

        # è¯„ä¼°å¼€å§‹æ—¶é—´
        start_time = time.time()

        print(f"\nğŸš€ å¼€å§‹è¯„ä¼° {dataset_tag} æ•°æ®é›†...")
        print("=" * 60)

        # æ‰¹é‡å¤„ç†æ•°æ®
        for i, item in enumerate(tqdm(dataset, desc="è¯„ä¼°è¿›åº¦")):
            instruction = item.get('instruction', '')
            expected_output = item.get('output', '').strip()

            if not instruction:
                print(f"âš ï¸  ç¬¬ {i+1} æ¡æ•°æ®ç¼ºå°‘instructionå­—æ®µï¼Œè·³è¿‡")
                continue

            # ç”Ÿæˆæ¨¡å‹å›å¤
            model_response = self.generate_response(instruction)

            # æå–ç­”æ¡ˆ
            extracted_answer = self.extract_answer(model_response)

            # åˆ¤æ–­æ˜¯å¦ä¸€è‡´
            is_correct = extracted_answer == expected_output

            # è®°å½•ç»“æœ
            result_item = {
                "question_id": i + 1,
                "instruction": instruction,
                "expected_answer": expected_output,
                "model_response": model_response,
                "extracted_answer": extracted_answer,
                "is_correct": is_correct,
                "timestamp": datetime.now().isoformat()
            }

            results.append(result_item)

            # ç”¨äºè®¡ç®—æŒ‡æ ‡ï¼ˆåªæœ‰æˆåŠŸæå–ç­”æ¡ˆçš„æ‰å‚ä¸è®¡ç®—ï¼‰
            if extracted_answer:
                predictions.append(extracted_answer)
                ground_truths.append(expected_output)

            # æ¯10æ¡æ•°æ®æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                current_accuracy = sum(1 for r in results if r['is_correct']) / len(results)
                print(f"ğŸ“Š å·²å¤„ç† {i+1}/{len(dataset)} æ¡ï¼Œå½“å‰å‡†ç¡®ç‡: {current_accuracy:.3f}")

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        evaluation_metrics = self.calculate_metrics(predictions, ground_truths)

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_questions = len(dataset)
        answered_questions = len([r for r in results if r['extracted_answer']])
        correct_answers = len([r for r in results if r['is_correct']])

        # è¯„ä¼°ç»“æŸæ—¶é—´
        end_time = time.time()
        evaluation_time = end_time - start_time

        # å‡†å¤‡æœ€ç»ˆç»“æœ
        final_results = {
            "dataset_info": {
                "dataset_tag": dataset_tag,
                "data_file": data_file,
                "total_questions": total_questions,
                "answered_questions": answered_questions,
                "unanswered_questions": total_questions - answered_questions,
            },
            "performance_metrics": evaluation_metrics,
            "statistics": {
                "overall_accuracy": correct_answers / total_questions if total_questions > 0 else 0,
                "answer_extraction_rate": answered_questions / total_questions if total_questions > 0 else 0,
                "evaluation_time_seconds": evaluation_time,
                "questions_per_second": total_questions / evaluation_time if evaluation_time > 0 else 0,
            },
            "timestamp": datetime.now().isoformat()
        }

        # ä¿å­˜è¯¦ç»†ç»“æœ
        answers_file = os.path.join(output_dir, f"generated_answers_{dataset_tag}.json")
        with open(answers_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = os.path.join(output_dir, f"eval_result_{dataset_tag}.json")
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœæ‘˜è¦
        self.print_evaluation_summary(final_results, answers_file, eval_file)

        return final_results

    def calculate_metrics(self, predictions: List[str], ground_truths: List[str]) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            ground_truths: çœŸå®æ ‡ç­¾åˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if not predictions or not ground_truths:
            return {
                "accuracy": 0.0,
                "precision_macro": 0.0,
                "recall_macro": 0.0,
                "f1_macro": 0.0,
                "precision_micro": 0.0,
                "recall_micro": 0.0,
                "f1_micro": 0.0,
                "per_class_metrics": {}
            }

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(ground_truths, predictions)

        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            ground_truths, predictions, average='macro', zero_division=0
        )

        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
            ground_truths, predictions, average='micro', zero_division=0
        )

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
            ground_truths, predictions, average=None, zero_division=0
        )

        # è·å–æ‰€æœ‰ç±»åˆ«
        unique_labels = sorted(list(set(ground_truths + predictions)))

        per_class_metrics = {}
        for i, label in enumerate(unique_labels):
            if i < len(precision_per_class):
                per_class_metrics[label] = {
                    "precision": float(precision_per_class[i]),
                    "recall": float(recall_per_class[i]),
                    "f1": float(f1_per_class[i]),
                    "support": int(support[i])
                }

        return {
            "accuracy": float(accuracy),
            "precision_macro": float(precision_macro),
            "recall_macro": float(recall_macro),
            "f1_macro": float(f1_macro),
            "precision_micro": float(precision_micro),
            "recall_micro": float(recall_micro),
            "f1_micro": float(f1_micro),
            "per_class_metrics": per_class_metrics
        }

    def print_evaluation_summary(self, results: Dict, answers_file: str, eval_file: str):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            answers_file: è¯¦ç»†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„
            eval_file: è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ğŸ‰ è¯„ä¼°å®Œæˆï¼ç»“æœæ‘˜è¦:")
        print("=" * 60)

        dataset_info = results["dataset_info"]
        metrics = results["performance_metrics"]
        stats = results["statistics"]

        print(f"ğŸ“Š æ•°æ®é›†: {dataset_info['dataset_tag']}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {dataset_info['data_file']}")
        print(f"ğŸ”¢ æ€»é—®é¢˜æ•°: {dataset_info['total_questions']}")
        print(f"âœ… æˆåŠŸå›ç­”: {dataset_info['answered_questions']}")
        print(f"âŒ æœªèƒ½å›ç­”: {dataset_info['unanswered_questions']}")

        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æ•´ä½“å‡†ç¡®ç‡: {stats['overall_accuracy']:.4f}")
        print(f"  ç­”æ¡ˆæå–ç‡: {stats['answer_extraction_rate']:.4f}")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision-Macro): {metrics['precision_macro']:.4f}")
        print(f"  å¬å›ç‡ (Recall-Macro): {metrics['recall_macro']:.4f}")
        print(f"  F1åˆ†æ•° (F1-Macro): {metrics['f1_macro']:.4f}")

        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"  è¯„ä¼°æ€»æ—¶é—´: {stats['evaluation_time_seconds']:.2f} ç§’")
        print(f"  å¤„ç†é€Ÿåº¦: {stats['questions_per_second']:.2f} é—®é¢˜/ç§’")

        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  è¯¦ç»†ç­”æ¡ˆ: {answers_file}")
        print(f"  è¯„ä¼°ç»“æœ: {eval_file}")

        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        if metrics["per_class_metrics"]:
            print(f"\nğŸ“Š å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
            for label, class_metrics in metrics["per_class_metrics"].items():
                print(f"  é€‰é¡¹ {label}: P={class_metrics['precision']:.3f}, "
                      f"R={class_metrics['recall']:.3f}, "
                      f"F1={class_metrics['f1']:.3f}, "
                      f"Support={class_metrics['support']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CultureSPAæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--dataset_id", type=int, required=True,
                       help="æ•°æ®é›†ID (2=CulturalBench, 3=normad, 4=cultureLLM, 5=cultureAtlas)")
    parser.add_argument("--data_file", type=str, required=True,
                       help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset_tag", type=str, required=True,
                       help="æ•°æ®é›†æ ‡ç­¾")
    parser.add_argument("--model_path", type=str, default=None,
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡è®¾ç½®")

    args = parser.parse_args()

    print("ğŸ›ï¸  CultureSPAæ¨¡å‹è¯„ä¼°å™¨")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset_tag} (ID: {args.dataset_id})")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {args.data_file}")
    print("=" * 60)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CultureSPAEvaluator(model_path=args.model_path, device=args.device)

    # åŠ è½½æ¨¡å‹
    if not evaluator.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè¯„ä¼°")
        return

    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(
        data_file=args.data_file,
        dataset_tag=args.dataset_tag,
        output_dir=args.output_dir
    )

    if results:
        print("âœ… è¯„ä¼°æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ è¯„ä¼°å¤±è´¥ï¼")


if __name__ == "__main__":
    main()